{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWYEon+WaWRMXHsBAEG7ZU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shusank8/Transformers/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlMNuiFh-P7k",
        "outputId": "be43513d-b2c5-41a3-cd2f-336d8d83aad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers.... Excited\n"
          ]
        }
      ],
      "source": [
        "print(\"Transformers.... Excited\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "s9kj-4Uh-U5j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config = {\n",
        "#     \"batch_size\":8,\n",
        "#     \"num_epochs\":20,\n",
        "#     \"lr\":10**-4,\n",
        "#     \"block_size\":512,\n",
        "#     \"embdim\":512,\n",
        "# }"
      ],
      "metadata": {
        "id": "eV07JB1yt8Js"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embdim):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embdim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embeddings(x)\n"
      ],
      "metadata": {
        "id": "1XEfUQea_NFA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, block_size, embdim, dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    pe = torch.zeros(block_size, embdim)\n",
        "\n",
        "    position = torch.arange(0, block_size, dtype = torch.float).unsqueeze(1)\n",
        "\n",
        "    div_term = torch.exp(torch.arange(0, embdim, 2).float() * (-math.log(10000.0)/embdim))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position*div_term)\n",
        "    pe[:, 1::2] = torch.cos(position*div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pe[:, :x.shape[1],:]\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "cBbwQQ9t_oO_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self, embdim):\n",
        "    super().__init__()\n",
        "    self.eps = 10**-6\n",
        "    self.alpha = nn.Parameter(torch.ones(embdim))\n",
        "    self.bias = nn.Parameter(torch.zeros(embdim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    xmean = x.mean(dim=-1, keepdim=True)\n",
        "    xvar = x.var(dim=-1, keepdim=True)\n",
        "    x = self.alpha*((x-xmean)/(xvar+self.eps)**(1/2))+self.bias\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "47y2lRPNJWO5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, embdim, dropout):\n",
        "    super().__init__()\n",
        "    self.m = nn.Sequential(\n",
        "        nn.Linear(embdim, 3*embdim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(3*embdim, embdim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.m(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "22KaQUo4d0IU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embdim, no_of_heads, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embdim = embdim\n",
        "    self.q = nn.Linear(embdim, embdim)\n",
        "    self.k = nn.Linear(embdim, embdim)\n",
        "    self.v = nn.Linear(embdim, embdim)\n",
        "    self.proj = nn.Linear(embdim, embdim)\n",
        "    self.no_of_heads = no_of_heads\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout):\n",
        "    head_dim = query.shape[-1]\n",
        "    attention_scores = (query@key.transpose(-2,-1))/math.sqrt(head_dim)\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "    attention_scores = attention_scores.softmax(dim=-1)\n",
        "    if dropout is not None:\n",
        "      attention_scores = dropout(attention_scores)\n",
        "    return (attention_scores@value), attention_scores\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, query, key, val, mask):\n",
        "    # for self attn query==key==val but cross attn\n",
        "    q = self.q(query)\n",
        "    k = self.k(key)\n",
        "    v = self.v(val)\n",
        "    hdim = q.shape[-1]//self.no_of_heads\n",
        "    # shape of q=> (B, T, C) BUT WE WANT TO BREAK C INTO DIFF HEADS\n",
        "    # (B,T,NO_OF_HEADS, HEADIM) WHERE NO_OF_HEADS * HEADIM = C\n",
        "    query = q.view(q.shape[0], q.shape[1], self.no_of_heads, hdim).transpose(1,2)\n",
        "    key = k.view(k.shape[0], k.shape[1], self.no_of_heads, hdim).transpose(1,2)\n",
        "    v = v.view(v.shape[0], v.shape[1], self.no_of_heads, hdim).transpose(1,2)\n",
        "\n",
        "    x, attn_scores = MultiHeadAttentionBlock(q, k, v, mask, self.dropout)\n",
        "    x = x.transpose(1,2).contiguous().view(x.shape[0], -1, self.embdim)\n",
        "    return self.proj(x)\n"
      ],
      "metadata": {
        "id": "2JJniu-nhLue"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self, dropout, embdim):\n",
        "    super().__init__()\n",
        "    self.dropout  = nn.Dropout(dropout)\n",
        "    self.norm = LayerNormalization(embdim)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x+ self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "9SBXl1nWrDN1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embdim, s_attn, ffwd, dropout):\n",
        "    super().__init__()\n",
        "    self.selfattn = s_attn\n",
        "    self.ffwd = ffwd\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout,embdim) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, src_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.selfattn(x,x,x,src_mask))\n",
        "    x = self.residual_connections[1](x, self.ffwd)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RoemyhuTri-H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layers, embdim):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(embdim)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "CUJf-6U5VruM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embdim, selfattn, crossattn, ffwd, dropout):\n",
        "    super().__init__()\n",
        "    self.selfattn = selfattn\n",
        "    self.crossattn = crossattn\n",
        "    self.ffwd = ffwd\n",
        "    self.residual_connections = nn.ModuleList([ResidualConnection(dropout,embdim) for _ in range(3)])\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    x = self.residual_connections[0](x, lambda x: self.selfattn(x,x,x, tgt_mask))\n",
        "    x = self.residual_connections[1](x, lambda x: self.crossattn(x, encoder_output, encoder_output, src_mask))\n",
        "    x = self.residual_connections[2](x, self.ffwd)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k7eGnV3iXdX6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layers, embdim):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization(embdim)\n",
        "\n",
        "  def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "8cxmS95SYn-O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, embdim, vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj = nn.Linear(embdim, vocab_size)\n",
        "  def forward(self, x):\n",
        "    return torch.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "MCt-sgmdZBzd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embd, src_pos, tgt_pos, projection_layer):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embd = src_embed\n",
        "    self.tgt_embd = tgt_embd\n",
        "    self.src_pos = src_pos\n",
        "    self.tgt_pos = tgt_pos\n",
        "    self.proj_layer = projection_layer\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    src = self.src_embd(src)\n",
        "    src = self.src_pos(src)\n",
        "    return self.encoder(src, src_mask)\n",
        "\n",
        "  def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "    tgt = self.tgt_embd(tgt)\n",
        "    tgt = self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "  def projection(self, x):\n",
        "    return self.projection_layer(x)\n"
      ],
      "metadata": {
        "id": "n5wYngTxZYs4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size,tgt_vocab_size, src_seq_len, tgt_seq_len, embdim, n_of_layers, no_of_heads, dropout):\n",
        "  src_embd = InputEmbeddings(src_vocab_size, embdim)\n",
        "  tgt_embd = InputEmbeddings(tgt_vocab_size, embdim)\n",
        "\n",
        "  src_pos = PositionalEmbeddings(src_seq_len, embdim, dropout)\n",
        "  tgt_pos = PositionalEmbeddings(tgt_seq_len, embdim, dropout)\n",
        "\n",
        "  encoder_blocks = []\n",
        "  for _ in range(n_of_layers):\n",
        "    encoder_sa = MultiHeadAttentionBlock(embdim, no_of_heads, dropout)\n",
        "    encoder_ffd = FeedForward(embdim, dropout)\n",
        "    encoder_block = EncoderBlock(embdim,encoder_sa, encoder_ffd, dropout)\n",
        "    encoder_blocks.append(encoder_block)\n",
        "  decoder_blocks = []\n",
        "  for _ in range(n_of_layers):\n",
        "    decoder_sa1 = MultiHeadAttentionBlock(embdim, no_of_heads, dropout)\n",
        "    decoder_ca = MultiHeadAttentionBlock(embdim, no_of_heads, dropout)\n",
        "    decoder_ffd = FeedForward(embdim, dropout)\n",
        "    decoder_block = DecoderBlock(embdim, decoder_sa1, decoder_ca, decoder_ffd, dropout)\n",
        "    decoder_blocks.append(decoder_block)\n",
        "\n",
        "  encoder = Encoder(nn.ModuleList(encoder_blocks), embdim)\n",
        "  decoder = Decoder(nn.ModuleList(decoder_blocks), embdim)\n",
        "\n",
        "  projection_layer = ProjectionLayer(embdim, tgt_vocab_size)\n",
        "\n",
        "  transformer = Transformer(encoder, decoder, src_embd, tgt_embd, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "  # initialize the parameters\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim()>=2:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "  return transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "EkePWWGPas1C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jsVDKc44oZm",
        "outputId": "e45939fe-3608-4299-d901-860e98e40701"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ni8vNmaj8YCv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"iamTangsang/Nepali-to-English-Translation-Dataset\")\n",
        "\n",
        "# Convert each split (train, test, validation) to Pandas DataFrame\n",
        "df_train = ds['train'].to_pandas() if 'train' in ds else None\n",
        "df_test = ds['test'].to_pandas() if 'test' in ds else None\n",
        "df_valid = ds['validation'].to_pandas() if 'validation' in ds else None\n"
      ],
      "metadata": {
        "id": "Cz6Sv66ODX5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e2e84b1-a67e-458f-d4d6-ce08fccf27ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([df_train, df_valid], ignore_index=True)"
      ],
      "metadata": {
        "id": "n6g3uc828Q23"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns = ['tgt', 'src']\n",
        "df_test.columns = ['tgt', 'src']"
      ],
      "metadata": {
        "id": "qBL8O9D36kCq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['tgt_len'] = df_train['tgt'].apply(lambda x : len(x.split(\" \")))"
      ],
      "metadata": {
        "id": "wpU0bo0jFMxH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['src_len'] = df_train['src'].apply(lambda x:len(x.split(\" \")))"
      ],
      "metadata": {
        "id": "pjp7qophAusz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_train = df_train[df_train['tgt_len']<100]"
      ],
      "metadata": {
        "id": "O3mSakQBA5Jn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train['src_len']\n",
        "df_train = df_train[df_train['src_len']<100]"
      ],
      "metadata": {
        "id": "ZIi_fV0FBigD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['tgt_len'] = df_test['tgt'].apply(lambda x : len(x.split(\" \")))\n",
        "df_test['src_len'] = df_test['src'].apply(lambda x:len(x.split(\" \")))\n",
        "df_test = df_test[df_test['tgt_len']<100]\n",
        "df_test = df_test[df_test['src_len']<100]"
      ],
      "metadata": {
        "id": "SA8e3GrbBvuC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE TEXT => ENGLISH\n",
        "# TARGET TEXT => NEPALI"
      ],
      "metadata": {
        "id": "3Q4s2U3dGIbu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.models import BPE, WordLevel\n",
        "import os\n",
        "def create_or_load_tokenizer(df):\n",
        "  if os.path.exists(\"SourceTokenizer.json\"):\n",
        "    srctok = Tokenizer.from_file(\"SourceTokenizer.json\")\n",
        "    tgttok = Tokenizer.from_file(\"TargetTokenizer.json\")\n",
        "    return srctok, tgttok\n",
        "  else:\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "    trainer = WordLevelTrainer(special_tokens=[\"[UNK]\",  \"[PAD]\", \"[SOS]\", \"[EOS]\"], vocab_size = 50000, min_frequency=8)\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    tokenizer.train_from_iterator(df_train['src'], trainer=trainer)\n",
        "    tokenizer.save(\"SourceTokenizer.json\")\n",
        "    tokenizer.train_from_iterator(df_train['tgt'], trainer=trainer)\n",
        "    tokenizer.save(\"TargetTokenizer.json\")\n",
        "    srctokenizer = Tokenizer.from_file(\"SourceTokenizer.json\")\n",
        "    targettokenizer = Tokenizer.from_file(\"TargetTokenizer.json\")\n",
        "    return srctokenizer, targettokenizer"
      ],
      "metadata": {
        "id": "BroBNjtW8bmO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_tok, tgt_tok = create_or_load_tokenizer(df_train)"
      ],
      "metadata": {
        "id": "AlplQDDc9oBC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_tok.get_vocab_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT5FqbOwAb-h",
        "outputId": "da90c8ef-0d1c-43b9-faa1-2ba678e89691"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20687"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_tok.get_vocab_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk9hOsyK9vzU",
        "outputId": "e26102ec-157c-49e5-a516-117e38047425"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36061"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class BiDataSet(Dataset):\n",
        "\n",
        "  def __init__(self, df, src_tok, tgt_tok,  block_size):\n",
        "    self.df = df\n",
        "    self.src_tok = src_tok\n",
        "    self.tgt_tok = tgt_tok\n",
        "    self.block_size = block_size\n",
        "\n",
        "    self.eos_tok = torch.tensor([src_tok.token_to_id(\"[EOS]\")], dtype = torch.int64)\n",
        "    self.sos_tok = torch.tensor([src_tok.token_to_id(\"[SOS]\")], dtype = torch.int64)\n",
        "    self.pad_tok = torch.tensor([src_tok.token_to_id(\"[PAD]\")], dtype = torch.int64)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    df = self.df.iloc[index]\n",
        "    src_text = df['src']\n",
        "    tgt_text = df['tgt']\n",
        "\n",
        "    enc_inp_tokens = self.src_tok.encode(src_text).ids\n",
        "    dec_inp_tokens = self.tgt_tok.encode(tgt_text).ids\n",
        "\n",
        "    enc_num_padding_tok = self.block_size - len(enc_inp_tokens)-2\n",
        "    dec_num_padding_tok = self.block_size - len(dec_inp_tokens)-1\n",
        "\n",
        "    if enc_num_padding_tok < 0 or dec_num_padding_tok<0:\n",
        "      raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "    encoder_input = torch.cat([\n",
        "        self.sos_tok,\n",
        "        torch.tensor(enc_inp_tokens, dtype = torch.int64),\n",
        "        self.eos_tok,\n",
        "        torch.tensor([self.pad_tok]*enc_num_padding_tok, dtype=torch.int64)\n",
        "    ])\n",
        "\n",
        "    decoder_input = torch.cat([\n",
        "        self.sos_tok,\n",
        "        torch.tensor(dec_inp_tokens, dtype = torch.int64),\n",
        "        torch.tensor([self.pad_tok]*dec_num_padding_tok, dtype=torch.int64)\n",
        "    ])\n",
        "\n",
        "    label = torch.cat(\n",
        "        [\n",
        "            torch.tensor(dec_inp_tokens, dtype = torch.int64),\n",
        "            self.eos_tok,\n",
        "            torch.tensor([self.pad_tok]*dec_num_padding_tok, dtype=torch.int64)\n",
        "\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    assert encoder_input.size(0)==self.block_size\n",
        "    assert decoder_input.size(0)==self.block_size\n",
        "    assert label.size(0)==self.block_size\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"encoder_input\":encoder_input,\n",
        "        \"decoder_input\":decoder_input,\n",
        "        \"encoder_mask\":(encoder_input!=self.pad_tok).unsqueeze(0).unsqueeze(0).int(),\n",
        "        \"decoder_mask\":(decoder_input!=self.pad_tok).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
        "        \"label\":label,\n",
        "        'src_text':src_text,\n",
        "        'tgt_text':tgt_text\n",
        "    }\n",
        "\n",
        "def causal_mask(size):\n",
        "  mask = torch.tril(torch.ones(1, size, size)).type(torch.int)\n",
        "  return mask==1\n"
      ],
      "metadata": {
        "id": "6gfr0BM-DxLA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train), len(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFh0aUkkgD1l",
        "outputId": "f1dd3eff-000a-4de3-873f-3eec4c298f95"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(713424, 10864)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# block_size = config['block_size']\n",
        "block_size = 164\n",
        "train_ds = BiDataSet(df_train,src_tok, tgt_tok, block_size )\n",
        "val_ds = BiDataSet(df_test,src_tok, tgt_tok, block_size )"
      ],
      "metadata": {
        "id": "EQyhoKnNhB1F"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len_src = 0\n",
        "# max_len_tgt = 0"
      ],
      "metadata": {
        "id": "fnqvsnBzhStJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = 0\n",
        "# ts = 0\n",
        "# for item in df_train['src']:\n",
        "#   src_ids = src_tok.encode(item).ids\n",
        "#   # tgt_ids = tgt_tok.encode(item['tgt']).ids\n",
        "#   max_len_src = max(max_len_src, len(src_ids))\n",
        "#   ts+=len(src_ids)\n",
        "#   # max_len_tgt = max(max_len_tgt, tgt_ids)"
      ],
      "metadata": {
        "id": "WixGLP7ihXqe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = 0\n",
        "# s = 0\n",
        "# for item in df_train['tgt']:\n",
        "#   # src_ids = src_tok.encode(item).ids\n",
        "#   tgt_ids = tgt_tok.encode(item).ids\n",
        "#   # max_len_src = max(max_len_src, len(src_ids))\n",
        "#   max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "#   s+=len(tgt_ids)"
      ],
      "metadata": {
        "id": "KFFdh2-4huAU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len_src, max_len_tgt"
      ],
      "metadata": {
        "id": "onZr4Ar2iKeh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_ds, batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "dmko40xOiw2z"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for x in iter(train_dataloader):\n",
        "#   inp = x\n",
        "#   break"
      ],
      "metadata": {
        "id": "MDT2whQapSxx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_transformer(src_tok.get_vocab_size(), tgt_tok.get_vocab_size(), 128, 128, 512, 6, 8, 0.2)\n",
        "model = model.to(\"cuda\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 10**-4, eps = 1e-9)"
      ],
      "metadata": {
        "id": "JiqDq5DTsFHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "24cd2ea7-42cb-4902-8cb2-2558d79154f3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'to'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-60b36114061e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBSxKIGmDGa8"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}